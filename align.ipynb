{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2ed630",
   "metadata": {},
   "source": [
    "# Chinese-Vietnamese Sentence Alignment using Vecalign + LaBSE\n",
    "\n",
    "- Uses the original [Vecalign](https://github.com/thompsonb/vecalign) repository for sentence alignment\n",
    "- Uses LaBSE (Language-agnostic BERT Sentence Embedding) for multilingual sentence embeddings\n",
    "- Performs sentence segmentation for both Chinese and Vietnamese\n",
    "- Supports many-to-many alignments (1:1, 1:2, 2:1, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf85443",
   "metadata": {},
   "source": [
    "## Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: pysbd in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: filelock in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: translate in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: sacrebleu in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: click in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from translate) (8.3.1)\n",
      "Requirement already satisfied: lxml in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from translate) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from translate) (2.32.5)\n",
      "Requirement already satisfied: libretranslatepy==2.1.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from translate) (2.1.1)\n",
      "Requirement already satisfied: portalocker in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: regex in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sacrebleu) (2025.11.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sacrebleu) (2.0.2)\n",
      "Requirement already satisfied: colorama in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->translate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->translate) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->translate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->translate) (2025.11.12)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: googletrans in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: httpx==0.13.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from googletrans) (0.13.3)\n",
      "Requirement already satisfied: certifi in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2025.11.12)\n",
      "Requirement already satisfied: hstspreload in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
      "Requirement already satisfied: chardet==3.* in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: rouge in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from rouge) (1.17.0)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Clone vecalign repository if not exists\n",
    "if not os.path.exists('vecalign'):\n",
    "    !git clone https://github.com/thompsonb/vecalign.git\n",
    "\n",
    "# Install required packages\n",
    "!pip install cython numpy sentence-transformers pysbd tqdm\n",
    "!pip install translate sacrebleu\n",
    "!pip install googletrans\n",
    "!pip install rouge\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859d2a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/thienan/Documents/coding/zh-vn-mt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the notebook's directory and set it as working directory\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# If we're inside 'vecalign' folder from installation, go back up\n",
    "if NOTEBOOK_DIR.name == 'vecalign' and (NOTEBOOK_DIR.parent / 'allignment.ipynb').exists():\n",
    "    NOTEBOOK_DIR = NOTEBOOK_DIR.parent\n",
    "    os.chdir(NOTEBOOK_DIR)\n",
    "\n",
    "print(f\"Working directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Add vecalign to path\n",
    "sys.path.insert(0, str(NOTEBOOK_DIR / 'vecalign'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b93bc",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee493eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_json(input_json_path, start_id, end_id, rename_cn_to_zh=False, source_name=None):\n",
    "    \"\"\"\n",
    "    Loads and filters a JSON file by ID range.\n",
    "    \n",
    "    Args:\n",
    "        input_json_path (str): Path to the source JSON file.\n",
    "        start_id (int): The starting ID of the range (inclusive).\n",
    "        end_id (int): The ending ID of the range (inclusive).\n",
    "        rename_cn_to_zh (bool): If True, rename 'cn' key to 'zh'.\n",
    "        source_name (str): Name of the source to add to each item.\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered data items.\n",
    "    \"\"\"\n",
    "    print(f\"Loading '{input_json_path}' for IDs between {start_id} and {end_id}...\")\n",
    "    \n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Error: The root of the JSON file is not a list.\")\n",
    "        return []\n",
    "\n",
    "    # Filter the data based on the id range\n",
    "    filtered_data = [\n",
    "        item for item in data\n",
    "        if isinstance(item, dict) and 'id' in item and start_id <= item.get('id', -1) <= end_id\n",
    "    ]\n",
    "    \n",
    "    # Rename 'cn' to 'zh' if needed\n",
    "    if rename_cn_to_zh:\n",
    "        for item in filtered_data:\n",
    "            if 'cn' in item:\n",
    "                item['zh'] = item.pop('cn')\n",
    "    \n",
    "    # Add source name if provided\n",
    "    if source_name:\n",
    "        for item in filtered_data:\n",
    "            item['source'] = source_name\n",
    "    \n",
    "    print(f\"  -> Loaded {len(filtered_data)} items\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def combine_json_subsets(output_path, *sources):\n",
    "    \"\"\"\n",
    "    Combines multiple JSON sources into a single file.\n",
    "    \n",
    "    Args:\n",
    "        output_path (str): Path to save the combined JSON file.\n",
    "        *sources: Tuples of (input_path, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for input_path, start_id, end_id, rename_cn_to_zh, source_name in sources:\n",
    "        items = load_and_filter_json(input_path, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "        combined_data.extend(items)\n",
    "    \n",
    "    # Write the combined data (keep original IDs)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\nSuccessfully created '{output_path}' with {len(combined_data)} total items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198b033",
   "metadata": {},
   "source": [
    "## Setup Sentence Splitters and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120273e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LaBSE model on cpu...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "import pysbd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Initialize sentence segmenters\n",
    "seg_zh = pysbd.Segmenter(language=\"zh\", clean=False)\n",
    "seg_en = pysbd.Segmenter(language=\"en\", clean=False)  # Use English rules for Vietnamese\n",
    "\n",
    "def split_chinese_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split Chinese text into sentences.\"\"\"\n",
    "    sentences = seg_zh.segment(text)\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1]\n",
    "\n",
    "def split_vietnamese_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split Vietnamese text into sentences.\"\"\"\n",
    "    sentences = seg_en.segment(text)\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1]\n",
    "\n",
    "# Load LaBSE model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading LaBSE model on {device}...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\", device=device)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06d1fb",
   "metadata": {},
   "source": [
    "## Setup Vecalign Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c41e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Import vecalign functions\n",
    "from dp_utils import make_alignment_types, vecalign, yield_overlaps\n",
    "\n",
    "print(\"Vecalign functions imported successfully!\")\n",
    "\n",
    "def preprocess_line(line):\n",
    "    \"\"\"Preprocess line same as vecalign does.\"\"\"\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        line = 'BLANK_LINE'\n",
    "    return line\n",
    "\n",
    "\n",
    "def layer(lines, num_overlaps, comb=' '):\n",
    "    \"\"\"\n",
    "    Make front-padded overlapping sentences (from vecalign).\n",
    "    \"\"\"\n",
    "    if num_overlaps < 1:\n",
    "        raise Exception('num_overlaps must be >= 1')\n",
    "    out = ['PAD', ] * min(num_overlaps - 1, len(lines))\n",
    "    for ii in range(len(lines) - num_overlaps + 1):\n",
    "        out.append(comb.join(lines[ii:ii + num_overlaps]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_doc_embedding_direct(sentences: List[str], num_overlaps: int = 4):\n",
    "    \"\"\"\n",
    "    Create document embedding matrix for vecalign directly.\n",
    "    Encodes all overlaps in a single batch for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        num_overlaps: Maximum number of sentences to concatenate\n",
    "        \n",
    "    Returns:\n",
    "        vecs0: 3D numpy array (num_overlaps, len(sentences), embedding_dim)\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return None\n",
    "    \n",
    "    # Preprocess sentences like vecalign does\n",
    "    lines = [preprocess_line(s) for s in sentences]\n",
    "    n_sents = len(lines)\n",
    "    \n",
    "    # Collect all overlaps we need to encode, with their positions\n",
    "    overlaps_to_encode = []\n",
    "    positions = []  # (overlap_idx, sent_idx)\n",
    "    \n",
    "    for ii, overlap in enumerate(range(1, num_overlaps + 1)):\n",
    "        layer_output = layer(lines, overlap)\n",
    "        for jj, out_line in enumerate(layer_output):\n",
    "            out_line = out_line[:10000]  # limit length\n",
    "            overlaps_to_encode.append(out_line)\n",
    "            positions.append((ii, jj))\n",
    "    \n",
    "    # Encode all overlaps in ONE batch call (much faster!)\n",
    "    if overlaps_to_encode:\n",
    "        all_embeddings = model.encode(\n",
    "            overlaps_to_encode, \n",
    "            convert_to_numpy=True, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=64\n",
    "        )\n",
    "        # Ensure it's a numpy array with correct dtype\n",
    "        all_embeddings = np.asarray(all_embeddings, dtype=np.float32)\n",
    "        vecsize = all_embeddings.shape[1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Create 3D array like vecalign expects\n",
    "    vecs0 = np.zeros((num_overlaps, n_sents, vecsize), dtype=np.float32)\n",
    "    \n",
    "    # Fill in the embeddings\n",
    "    for idx, (ii, jj) in enumerate(positions):\n",
    "        vecs0[ii, jj, :] = all_embeddings[idx]\n",
    "    \n",
    "    # Normalize vectors to unit length (required for vecalign cosine similarity)\n",
    "    for ii in range(num_overlaps):\n",
    "        for jj in range(n_sents):\n",
    "            norm = np.linalg.norm(vecs0[ii, jj, :])\n",
    "            if norm > 1e-8:\n",
    "                vecs0[ii, jj, :] = vecs0[ii, jj, :] / norm\n",
    "            else:\n",
    "                # Use small random vector if norm is too small\n",
    "                vecs0[ii, jj, :] = np.random.randn(vecsize).astype(np.float32)\n",
    "                vecs0[ii, jj, :] = vecs0[ii, jj, :] / np.linalg.norm(vecs0[ii, jj, :])\n",
    "    \n",
    "    return vecs0\n",
    "\n",
    "print(\"Vecalign functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d72ee",
   "metadata": {},
   "source": [
    "## Define Alignment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b073965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment function defined!\n"
     ]
    }
   ],
   "source": [
    "def align_zh_vi(zh_text: str, vi_text: str, \n",
    "                source: str = \"unknown\", \n",
    "                source_id: int = 0,\n",
    "                alignment_max_size: int = 4,\n",
    "                del_percentile_frac: float = 0.2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Aligns Chinese and Vietnamese text using Vecalign with LaBSE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zh_text : str\n",
    "        Chinese text to align\n",
    "    vi_text : str\n",
    "        Vietnamese text to align\n",
    "    source : str\n",
    "        Source identifier for the text pair\n",
    "    source_id : int\n",
    "        ID of the source document\n",
    "    alignment_max_size : int\n",
    "        Maximum sentences in one alignment (default: 4)\n",
    "    del_percentile_frac : float\n",
    "        Deletion penalty percentile (default: 0.2)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]: List of aligned sentence pairs with metadata\n",
    "    \"\"\"\n",
    "    from math import ceil\n",
    "    \n",
    "    # Split into sentences\n",
    "    zh_sents = split_chinese_sentences(zh_text)\n",
    "    vi_sents = split_vietnamese_sentences(vi_text)\n",
    "    \n",
    "    if not zh_sents or not vi_sents:\n",
    "        return []\n",
    "    \n",
    "    # Create document embeddings directly (single batch encode for each side)\n",
    "    vecs0 = make_doc_embedding_direct(zh_sents, alignment_max_size)\n",
    "    vecs1 = make_doc_embedding_direct(vi_sents, alignment_max_size)\n",
    "    \n",
    "    if vecs0 is None or vecs1 is None:\n",
    "        return []\n",
    "    \n",
    "    # Get alignment types\n",
    "    final_alignment_types = make_alignment_types(alignment_max_size)\n",
    "    \n",
    "    # Calculate search width\n",
    "    width_over2 = ceil(alignment_max_size / 2.0) + 5  # 5 is search_buffer_size\n",
    "    \n",
    "    # Run vecalign\n",
    "    stack = vecalign(\n",
    "        vecs0=vecs0,\n",
    "        vecs1=vecs1,\n",
    "        final_alignment_types=final_alignment_types,\n",
    "        del_percentile_frac=del_percentile_frac,\n",
    "        width_over2=width_over2,\n",
    "        max_size_full_dp=300,\n",
    "        costs_sample_size=20000,\n",
    "        num_samps_for_norm=100\n",
    "    )\n",
    "    \n",
    "    # Extract alignments\n",
    "    alignments = stack[0]['final_alignments']\n",
    "    scores = stack[0]['alignment_scores']\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    for (src_indices, tgt_indices), score in zip(alignments, scores):\n",
    "        if len(src_indices) > 0 and len(tgt_indices) > 0:\n",
    "            zh_aligned = ' '.join(zh_sents[i] for i in src_indices)\n",
    "            vi_aligned = ' '.join(vi_sents[i] for i in tgt_indices)\n",
    "            \n",
    "            results.append({\n",
    "                'src_lang': zh_aligned,\n",
    "                'tgt_lang': vi_aligned,\n",
    "                'src_id': int(source_id),\n",
    "                'align_type': f\"{len(src_indices)}-{len(tgt_indices)}\",\n",
    "                'score': float(score),\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Alignment function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fa19a",
   "metadata": {},
   "source": [
    "## Process Data from JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a JSON file containing zh-vi text pairs and create aligned corpus.\n",
    "    Saves results incrementally to JSON after each successful alignment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_path : str\n",
    "        Path to input JSON file\n",
    "    output_path : str\n",
    "        Path to output JSON file (results saved incrementally)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]: All aligned pairs (with full metadata for statistics)\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import csv\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} documents\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    \n",
    "    all_pairs: List[Dict] = []\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Aligning documents\"):\n",
    "        if 'zh' not in item or 'vi' not in item:\n",
    "            continue\n",
    "        \n",
    "        zh_text = item['zh'].strip()\n",
    "        vi_text = item['vi'].strip()\n",
    "        \n",
    "        if not zh_text or not vi_text:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            aligned = align_zh_vi(\n",
    "                zh_text, vi_text,\n",
    "                source=item.get('source', 'unknown'),\n",
    "                source_id=item.get('id', 0),\n",
    "                alignment_max_size = 2\n",
    "            )\n",
    "            \n",
    "            if aligned:\n",
    "                all_pairs.extend(aligned)\n",
    "                \n",
    "                # Filter keys for saving (only keep src_lang, tgt_lang, src_id)\n",
    "                # We keep the full data in 'all_pairs' for statistics calculation\n",
    "                pairs_to_save = [\n",
    "                    {\n",
    "                        'src_id': p['src_id'],\n",
    "                        'zh': p['src_lang'],\n",
    "                        'vi': p['tgt_lang']\n",
    "                    }\n",
    "                    for p in all_pairs if (1.0 - p.get('score', 0)) >= 0.6 \n",
    "                ]\n",
    "                \n",
    "                # Save incrementally after each successful alignment\n",
    "                with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=['src_id', 'zh', 'vi'])\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(pairs_to_save)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {item.get('id', '?')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal aligned pairs: {len(all_pairs)}\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352821f",
   "metadata": {},
   "source": [
    "## Define src_name, from_id, to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c105c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cq3 = [\n",
    "    {\n",
    "        \"src_name\": \"json1.json\",\n",
    "        \"from_id\": 1,\n",
    "        \"to_id\": 212\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"src_name\": \"json2.json\",\n",
    "        \"from_id\": 1,\n",
    "        \"to_id\": 1163\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"src_name\": \"pdf1.json\",\n",
    "        \"from_id\": 1,\n",
    "        \"to_id\": 125\n",
    "    },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1036b",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23216d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing json1.json (IDs 1-212) ===\n",
      "Loading 'json1.json' for IDs between 1 and 212...\n",
      "  -> Loaded 212 items\n",
      "\n",
      "Successfully created 'data.json' with 212 total items.\n",
      "Loaded 212 documents\n",
      "Output: /home/thienan/Documents/coding/zh-vn-mt/corpus/json1_1_212.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents: 100%|██████████| 212/212 [1:03:55<00:00, 18.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total aligned pairs: 9621\n",
      "Saved to: /home/thienan/Documents/coding/zh-vn-mt/corpus/json1_1_212.csv\n",
      "\n",
      "--- Statistics for json1.json (1-212) ---\n",
      "Aligned pairs: 9621\n",
      "Alignment type distribution:\n",
      "  1-1: 9621 (100.0%)\n",
      "Score statistics:\n",
      "  Min: 0.0000\n",
      "  Max: 1.2758\n",
      "  Mean: 0.3875\n",
      "\n",
      "=== Processing json2.json (IDs 1-1163) ===\n",
      "Loading 'json2.json' for IDs between 1 and 1163...\n",
      "  -> Loaded 1163 items\n",
      "\n",
      "Successfully created 'data.json' with 1163 total items.\n",
      "Loaded 1163 documents\n",
      "Output: /home/thienan/Documents/coding/zh-vn-mt/corpus/json2_1_1163.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents: 100%|██████████| 1163/1163 [12:08<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total aligned pairs: 2075\n",
      "Saved to: /home/thienan/Documents/coding/zh-vn-mt/corpus/json2_1_1163.csv\n",
      "\n",
      "--- Statistics for json2.json (1-1163) ---\n",
      "Aligned pairs: 2075\n",
      "Alignment type distribution:\n",
      "  1-1: 2075 (100.0%)\n",
      "Score statistics:\n",
      "  Min: 0.0317\n",
      "  Max: 1.0538\n",
      "  Mean: 0.3149\n",
      "\n",
      "=== Processing pdf1.json (IDs 1-125) ===\n",
      "Loading 'pdf1.json' for IDs between 1 and 125...\n",
      "  -> Loaded 125 items\n",
      "\n",
      "Successfully created 'data.json' with 125 total items.\n",
      "Loaded 125 documents\n",
      "Output: /home/thienan/Documents/coding/zh-vn-mt/corpus/pdf1_1_125.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents: 100%|██████████| 125/125 [02:17<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total aligned pairs: 264\n",
      "Saved to: /home/thienan/Documents/coding/zh-vn-mt/corpus/pdf1_1_125.csv\n",
      "\n",
      "--- Statistics for pdf1.json (1-125) ---\n",
      "Aligned pairs: 264\n",
      "Alignment type distribution:\n",
      "  1-1: 264 (100.0%)\n",
      "Score statistics:\n",
      "  Min: 0.1724\n",
      "  Max: 1.2125\n",
      "  Mean: 0.6597\n",
      "\n",
      "All processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for item in cq3:\n",
    "    all_pairs = []\n",
    "\n",
    "    src_name = item[\"src_name\"]\n",
    "    from_id = item[\"from_id\"]\n",
    "    to_id = item[\"to_id\"]\n",
    "    \n",
    "    # Determine parameters based on file name\n",
    "    # json1.json used True in previous examples, others used False\n",
    "    rename_cn_to_zh = (src_name == \"json1.json\")\n",
    "    source_label = src_name.replace(\".json\", \"\")\n",
    "    \n",
    "    output_file = f'corpus/{source_label}_{from_id}_{to_id}.csv'\n",
    "    if (NOTEBOOK_DIR / output_file).exists():\n",
    "        print(f\"Skipping {output_file} (already exists)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=== Processing {src_name} (IDs {from_id}-{to_id}) ===\")\n",
    "    \n",
    "    # Create subset data.json\n",
    "    combine_json_subsets(\n",
    "        \"data.json\",\n",
    "        (src_name, from_id, to_id, rename_cn_to_zh, source_label)\n",
    "    )\n",
    "    \n",
    "    # Run alignment\n",
    "    pairs = process_data(\n",
    "        str(NOTEBOOK_DIR / 'data.json'), \n",
    "        output_path=str(NOTEBOOK_DIR / output_file)\n",
    "    )\n",
    "    \n",
    "    # --- Statistics for this batch before cleaning ---\n",
    "    if pairs:\n",
    "        print(f\"\\n--- Statistics for {src_name} ({from_id}-{to_id}) ---\")\n",
    "        print(f\"Aligned pairs: {len(pairs)}\")\n",
    "        \n",
    "        # Alignment type distribution\n",
    "        type_counts = {}\n",
    "        for p in pairs:\n",
    "            atype = p.get('align_type', 'unknown')\n",
    "            type_counts[atype] = type_counts.get(atype, 0) + 1\n",
    "        \n",
    "        print(\"Alignment type distribution:\")\n",
    "        for atype, count in sorted(type_counts.items()):\n",
    "            pct = 100 * count / len(pairs)\n",
    "            print(f\"  {atype}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        # Score statistics\n",
    "        scores = [p.get('score', 0) for p in pairs]\n",
    "        if scores:\n",
    "            print(f\"Score statistics:\")\n",
    "            print(f\"  Min: {min(scores):.4f}\")\n",
    "            print(f\"  Max: {max(scores):.4f}\")\n",
    "            print(f\"  Mean: {np.mean(scores):.4f}\")\n",
    "    # ---------------------------------\n",
    "    \n",
    "    # all_pairs.extend(pairs)\n",
    "\n",
    "# print(f\"\\nAll processing complete. Total pairs accumulated: {len(all_pairs)}\")\n",
    "print(f\"\\nAll processing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc03a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading source files...\n",
      "  Loaded 5960 pairs from json1_1_212.csv -> Selected 50\n",
      "  Loaded 1649 pairs from json2_1_1163.csv -> Selected 50\n",
      "  Loaded 16 pairs from pdf1_1_125.csv -> Selected 15\n",
      "Total loaded pairs: 115\n",
      "Starting evaluation on 115 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 32/115 [01:03<04:29,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sample 32: The read operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [03:18<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 0.37\n",
      "chrF Score: 0.55\n",
      "ROUGE-1: 0.6593\n",
      "ROUGE-2: 0.4461\n",
      "ROUGE-L: 0.6191\n",
      "Average Semantic Similarity: 0.9077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from googletrans import Translator\n",
    "import sacrebleu\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def translate_text(text, src_lang, tgt_lang):\n",
    "    translator = Translator()\n",
    "    result = translator.translate(text, src = src_lang, dest = tgt_lang)\n",
    "    if hasattr(result, '__await__'):\n",
    "        result = await result\n",
    "    return result.text\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "similarities = []\n",
    "\n",
    "# # Limit the number of samples for evaluation\n",
    "num_samples_to_evaluate = 50\n",
    "\n",
    "# Load all generated files based on cq3 list\n",
    "full_corpus = []\n",
    "print(\"Loading source files...\")\n",
    "\n",
    "# Ensure cq3 is available (it should be from previous cells)\n",
    "if 'cq3' not in locals():\n",
    "    print(\"Error: 'cq3' list not found. Please run the cell defining 'cq3' first.\")\n",
    "else:\n",
    "    for item in cq3:\n",
    "        src_name = item[\"src_name\"]\n",
    "        from_id = item[\"from_id\"]\n",
    "        to_id = item[\"to_id\"]\n",
    "        source_label = src_name.replace(\".json\", \"\")\n",
    "        filename = f'{source_label}_{from_id}_{to_id}.csv'\n",
    "        file_path = NOTEBOOK_DIR / 'corpus' / filename\n",
    "        \n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                data = list(reader)\n",
    "\n",
    "                # Filter valid pairs (length >= 10)\n",
    "                valid_data = [p for p in data if len(p.get('zh', '')) >= 10]\n",
    "                \n",
    "                # Select up to 50 samples per file\n",
    "                if len(valid_data) > num_samples_to_evaluate:\n",
    "                    selected_data = random.sample(valid_data, num_samples_to_evaluate)\n",
    "                else:\n",
    "                    selected_data = valid_data\n",
    "                \n",
    "                full_corpus.extend(selected_data)\n",
    "                print(f\"  Loaded {len(data)} pairs from {filename} -> Selected {len(selected_data)}\")\n",
    "        else:\n",
    "            print(f\"  Warning: File {filename} not found\")\n",
    "\n",
    "print(f\"Total loaded pairs: {len(full_corpus)}\")\n",
    "\n",
    "if not full_corpus:\n",
    "    print(\"No data loaded. Aborting evaluation.\")\n",
    "else:\n",
    "    # Filter for valid pairs (length > 10)\n",
    "    # Note: keys are 'zh' and 'vi' in the output files\n",
    "    valid_pairs = [p for p in full_corpus if len(p.get('zh', '')) >= 10]\n",
    "    if not valid_pairs:\n",
    "        print(\"No valid pairs found (length >= 10). Using all pairs.\")\n",
    "        valid_pairs = full_corpus\n",
    "\n",
    "    # Sort by length to pick longer sentences\n",
    "    # sorted_corpus = sorted(valid_pairs, key=lambda x: len(x.get('zh', '')), reverse=True)\n",
    "    # test_data = sorted_corpus[:num_samples_to_evaluate]\n",
    "\n",
    "    print(f\"Starting evaluation on {len(valid_pairs)} samples...\")\n",
    "\n",
    "    for i, sample in enumerate(tqdm(valid_pairs)):\n",
    "        zh_text = sample.get(\"zh\", \"\")\n",
    "        vi_reference = sample.get(\"vi\", \"\")\n",
    "        \n",
    "        if not zh_text:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Translate ZH -> VI\n",
    "            vi_translated = await translate_text(zh_text,src_lang='zh-cn',tgt_lang='vi')\n",
    "            \n",
    "            predictions.append(vi_translated)\n",
    "            references.append(vi_reference)\n",
    "            \n",
    "            # Calculate semantic similarity using the loaded model\n",
    "            embeddings = model.encode([vi_reference, vi_translated])\n",
    "            sim = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "            similarities.append(sim)\n",
    "\n",
    "            # print(f\"Sample {i+1}:\")\n",
    "            # print(f\"  ZH: {zh_text}\")\n",
    "            # print(f\"  Ref: {vi_reference}\")\n",
    "            # print(f\"  MT:  {vi_translated}\")\n",
    "            # print(f\"  Sim: {sim:.4f}\")\n",
    "            # print(\"-\" * 30)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i+1}: {e}\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    if references and predictions:\n",
    "        # BLEU\n",
    "        bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "        print(f\"\\nBLEU Score: {bleu.score/100:.2f}\")\n",
    "\n",
    "        # chrF\n",
    "        chrf = sacrebleu.corpus_chrf(predictions, [references])\n",
    "        print(f\"chrF Score: {chrf.score/100:.2f}\")\n",
    "        \n",
    "        # ROUGE\n",
    "        try:\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(predictions, references, avg=True)\n",
    "            print(f\"ROUGE-1: {scores['rouge-1']['f']:.4f}\")\n",
    "            print(f\"ROUGE-2: {scores['rouge-2']['f']:.4f}\")\n",
    "            print(f\"ROUGE-L: {scores['rouge-l']['f']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE: {e}\")\n",
    "        \n",
    "        # Average Semantic Similarity\n",
    "        avg_sim = np.mean(similarities)\n",
    "        print(f\"Average Semantic Similarity: {avg_sim:.4f}\")\n",
    "    else:\n",
    "        print(\"No successful predictions to calculate metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b87f6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading generated alignment files for semantic analysis...\n",
      "Calculating semantic similarity for 7625 pairs...\n",
      "Encoding source sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 120/120 [04:31<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding target sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 120/120 [05:15<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarities...\n",
      "\n",
      "=== Semantic Similarity Statistics (LaBSE) ===\n",
      "Count:  7625\n",
      "Mean:   0.8557\n",
      "Median: 0.8501\n",
      "Std:    0.0670\n",
      "Min:    0.6458\n",
      "Max:    1.0000\n",
      "\n",
      "Percentiles:\n",
      "  1%: 0.7232\n",
      "  5%: 0.7511\n",
      "  10%: 0.7706\n",
      "  25%: 0.8038\n",
      "  50%: 0.8501\n",
      "  75%: 0.9095\n",
      "  90%: 0.9461\n",
      "  95%: 0.9684\n",
      "  99%: 0.9870\n",
      "\n",
      "Distribution:\n",
      "  0.0 - 0.1: 0\n",
      "  0.1 - 0.2: 0\n",
      "  0.2 - 0.3: 0\n",
      "  0.3 - 0.4: 0\n",
      "  0.4 - 0.5: 0\n",
      "  0.5 - 0.6: 0\n",
      "  0.6 - 0.7: 13\n",
      "  0.7 - 0.8: 1757\n",
      "  0.8 - 0.9: 3700\n",
      "  0.9 - 1.0: 2126\n"
     ]
    }
   ],
   "source": [
    "def semantic_statistics(config_list, base_dir, model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Calculates semantic similarity statistics for the entire corpus.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import csv\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    full_corpus = []\n",
    "    print(\"Loading generated alignment files for semantic analysis...\")\n",
    "    \n",
    "    # Load data\n",
    "    for item in config_list:\n",
    "        src_name = item[\"src_name\"]\n",
    "        from_id = item[\"from_id\"]\n",
    "        to_id = item[\"to_id\"]\n",
    "        source_label = src_name.replace(\".json\", \"\")\n",
    "        filename = f'{source_label}_{from_id}_{to_id}.csv'\n",
    "        file_path = base_dir / 'corpus' / filename\n",
    "        \n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                data = list(reader)\n",
    "                full_corpus.extend(data)\n",
    "    \n",
    "    if not full_corpus:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Calculating semantic similarity for {len(full_corpus)} pairs...\")\n",
    "    \n",
    "    src_texts = [p['zh'] for p in full_corpus]\n",
    "    tgt_texts = [p['vi'] for p in full_corpus]\n",
    "    \n",
    "    # Encode in batches\n",
    "    print(\"Encoding source sentences...\")\n",
    "    src_embeddings = model.encode(src_texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    print(\"Encoding target sentences...\")\n",
    "    tgt_embeddings = model.encode(tgt_texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    # Compute Cosine Similarity\n",
    "    print(\"Computing similarities...\")\n",
    "    \n",
    "    # Normalize embeddings to unit length\n",
    "    norms_src = np.linalg.norm(src_embeddings, axis=1, keepdims=True)\n",
    "    norms_tgt = np.linalg.norm(tgt_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    norms_src[norms_src == 0] = 1e-10\n",
    "    norms_tgt[norms_tgt == 0] = 1e-10\n",
    "    \n",
    "    src_embeddings_norm = src_embeddings / norms_src\n",
    "    tgt_embeddings_norm = tgt_embeddings / norms_tgt\n",
    "    \n",
    "    # Dot product of corresponding vectors (row-wise)\n",
    "    similarities = np.sum(src_embeddings_norm * tgt_embeddings_norm, axis=1)\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n=== Semantic Similarity Statistics (LaBSE) ===\")\n",
    "    print(f\"Count:  {len(similarities)}\")\n",
    "    print(f\"Mean:   {np.mean(similarities):.4f}\")\n",
    "    print(f\"Median: {np.median(similarities):.4f}\")\n",
    "    print(f\"Std:    {np.std(similarities):.4f}\")\n",
    "    print(f\"Min:    {np.min(similarities):.4f}\")\n",
    "    print(f\"Max:    {np.max(similarities):.4f}\")\n",
    "    \n",
    "    # Percentiles\n",
    "    percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "    print(\"\\nPercentiles:\")\n",
    "    for p in percentiles:\n",
    "        print(f\"  {p}%: {np.percentile(similarities, p):.4f}\")\n",
    "        \n",
    "    # Simple Histogram\n",
    "    print(\"\\nDistribution:\")\n",
    "    hist, bin_edges = np.histogram(similarities, bins=10, range=(0, 1))\n",
    "    for i in range(len(hist)):\n",
    "        print(f\"  {bin_edges[i]:.1f} - {bin_edges[i+1]:.1f}: {hist[i]}\")\n",
    "\n",
    "# Run the statistics\n",
    "if 'cq3' in locals() and 'model' in locals():\n",
    "    semantic_statistics(cq3, NOTEBOOK_DIR, model)\n",
    "else:\n",
    "    print(\"Error: Missing 'cq3' config or 'model'. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ee6c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading generated alignment files...\n",
      "  Loaded 5960 pairs from json1_1_212.csv\n",
      "  Loaded 1649 pairs from json2_1_1163.csv\n",
      "  Loaded 16 pairs from pdf1_1_125.csv\n",
      "Total loaded pairs: 7625\n",
      "\n",
      "--- Alignment Statistics---\n",
      "  1-1: 7523 (98.7%)\n",
      "  1-2: 101 (1.3%)\n",
      "  1-3: 1 (0.0%)\n",
      "--------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def allignment_statistics(config_list, base_dir):\n",
    "    import csv\n",
    "    full_corpus = []\n",
    "\n",
    "    print(\"Loading generated alignment files...\")\n",
    "    \n",
    "    for item in config_list:\n",
    "        src_name = item[\"src_name\"]\n",
    "        from_id = item[\"from_id\"]\n",
    "        to_id = item[\"to_id\"]\n",
    "        source_label = src_name.replace(\".json\", \"\")\n",
    "        filename = f'{source_label}_{from_id}_{to_id}.csv'\n",
    "        file_path = base_dir / 'corpus' / filename\n",
    "        \n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                data = list(reader)\n",
    "                \n",
    "                # Infer alignment type dynamically since it's not saved in JSON\n",
    "                for entry in data:\n",
    "                    # Use the global split functions defined earlier\n",
    "                    src_sents = split_chinese_sentences(entry.get('zh', ''))\n",
    "                    tgt_sents = split_vietnamese_sentences(entry.get('vi', ''))\n",
    "                    entry['align_type'] = f\"{len(src_sents)}-{len(tgt_sents)}\"\n",
    "                \n",
    "                full_corpus.extend(data)\n",
    "                print(f\"  Loaded {len(data)} pairs from {filename}\")\n",
    "        else:\n",
    "            print(f\"  Warning: File {filename} not found\")\n",
    "\n",
    "    print(f\"Total loaded pairs: {len(full_corpus)}\")\n",
    "\n",
    "    if not full_corpus:\n",
    "        print(\"No data loaded. Aborting evaluation.\")\n",
    "        return\n",
    "\n",
    "    # --- Calculate and Print Statistics ---\n",
    "    print(\"\\n--- Alignment Statistics---\")\n",
    "    type_counts = {}\n",
    "    for p in full_corpus:\n",
    "        atype = p.get('align_type', 'unknown')\n",
    "        type_counts[atype] = type_counts.get(atype, 0) + 1\n",
    "    \n",
    "    for atype, count in sorted(type_counts.items()):\n",
    "        pct = 100 * count / len(full_corpus)\n",
    "        print(f\"  {atype}: {count} ({pct:.1f}%)\")\n",
    "    print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "allignment_statistics(cq3, NOTEBOOK_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
