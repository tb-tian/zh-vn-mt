{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2ed630",
   "metadata": {},
   "source": [
    "# Chinese-Vietnamese Sentence Alignment using Vecalign + SBERT\n",
    "\n",
    "## Features:\n",
    "- Uses the original [Vecalign](https://github.com/thompsonb/vecalign) repository for sentence alignment\n",
    "- Uses LaBSE (Language-agnostic BERT Sentence Embedding) for multilingual sentence embeddings (instead of LASER)\n",
    "- Performs sentence segmentation for both Chinese and Vietnamese\n",
    "- Supports many-to-many alignments (1:1, 1:2, 2:1, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf85443",
   "metadata": {},
   "source": [
    "## 0. Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b38d5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vecalign'...\n",
      "remote: Enumerating objects: 63, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Enumerating objects: 63, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 63 (delta 4), reused 8 (delta 4), pack-reused 51 (from 1)\u001b[K\n",
      "Receiving objects: 100% (63/63), 109.05 MiB | 5.38 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n",
      "remote: Total 63 (delta 4), reused 8 (delta 4), pack-reused 51 (from 1)\u001b[K\n",
      "Receiving objects: 100% (63/63), 109.05 MiB | 5.38 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n",
      "Updating files: 100% (41/41), done.\n",
      "Updating files: 100% (41/41), done.\n",
      "Requirement already satisfied: cython in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: pysbd in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: cython in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: sentence-transformers in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: pysbd in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: filelock in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: filelock in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Installation complete!\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Clone vecalign repository if not exists\n",
    "if not os.path.exists('vecalign'):\n",
    "    !git clone https://github.com/thompsonb/vecalign.git\n",
    "\n",
    "# Install required packages\n",
    "!pip install cython numpy sentence-transformers pysbd tqdm\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "859d2a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/thienan/Documents/coding/zh-vn-mt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the notebook's directory and set it as working directory\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# If we're inside 'vecalign' folder from installation, go back up\n",
    "if NOTEBOOK_DIR.name == 'vecalign' and (NOTEBOOK_DIR.parent / 'vecalign_sbert_notebook.ipynb').exists():\n",
    "    NOTEBOOK_DIR = NOTEBOOK_DIR.parent\n",
    "    os.chdir(NOTEBOOK_DIR)\n",
    "\n",
    "print(f\"Working directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Add vecalign to path\n",
    "sys.path.insert(0, str(NOTEBOOK_DIR / 'vecalign'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b93bc",
   "metadata": {},
   "source": [
    "## 1. Create Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee493eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_json(input_json_path, start_id, end_id, rename_cn_to_zh=False, source_name=None):\n",
    "    \"\"\"\n",
    "    Loads and filters a JSON file by ID range.\n",
    "    \n",
    "    Args:\n",
    "        input_json_path (str): Path to the source JSON file.\n",
    "        start_id (int): The starting ID of the range (inclusive).\n",
    "        end_id (int): The ending ID of the range (inclusive).\n",
    "        rename_cn_to_zh (bool): If True, rename 'cn' key to 'zh'.\n",
    "        source_name (str): Name of the source to add to each item.\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered data items.\n",
    "    \"\"\"\n",
    "    print(f\"Loading '{input_json_path}' for IDs between {start_id} and {end_id}...\")\n",
    "    \n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Error: The root of the JSON file is not a list.\")\n",
    "        return []\n",
    "\n",
    "    # Filter the data based on the id range\n",
    "    filtered_data = [\n",
    "        item for item in data\n",
    "        if isinstance(item, dict) and 'id' in item and start_id <= item.get('id', -1) <= end_id\n",
    "    ]\n",
    "    \n",
    "    # Rename 'cn' to 'zh' if needed\n",
    "    if rename_cn_to_zh:\n",
    "        for item in filtered_data:\n",
    "            if 'cn' in item:\n",
    "                item['zh'] = item.pop('cn')\n",
    "    \n",
    "    # Add source name if provided\n",
    "    if source_name:\n",
    "        for item in filtered_data:\n",
    "            item['source'] = source_name\n",
    "    \n",
    "    print(f\"  -> Loaded {len(filtered_data)} items\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def combine_json_subsets(output_path, *sources):\n",
    "    \"\"\"\n",
    "    Combines multiple JSON sources into a single file.\n",
    "    \n",
    "    Args:\n",
    "        output_path (str): Path to save the combined JSON file.\n",
    "        *sources: Tuples of (input_path, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for input_path, start_id, end_id, rename_cn_to_zh, source_name in sources:\n",
    "        items = load_and_filter_json(input_path, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "        combined_data.extend(items)\n",
    "    \n",
    "    # Write the combined data (keep original IDs)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\nSuccessfully created '{output_path}' with {len(combined_data)} total items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7efe7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.json already exists, skipping creation.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"data.json\"):\n",
    "    combine_json_subsets(\n",
    "        \"data.json\",\n",
    "        # (input_file, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "        (\"json1.json\", 1, 212, True, \"json1\"),   # json1 uses 'cn' -> rename to 'zh'\n",
    "        (\"json2.json\", 1, 1163, False, \"json2\"), # json2 already uses 'zh'\n",
    "    )\n",
    "else:\n",
    "    print(\"data.json already exists, skipping creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198b033",
   "metadata": {},
   "source": [
    "## 2. Setup Sentence Splitters and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120273e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LaBSE model on cpu...\n",
      "Model loaded!\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "import pysbd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Initialize sentence segmenters\n",
    "seg_zh = pysbd.Segmenter(language=\"zh\", clean=False)\n",
    "seg_en = pysbd.Segmenter(language=\"en\", clean=False)  # Use English rules for Vietnamese\n",
    "\n",
    "def split_chinese_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split Chinese text into sentences.\"\"\"\n",
    "    sentences = seg_zh.segment(text)\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1]\n",
    "\n",
    "def split_vietnamese_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Split Vietnamese text into sentences.\"\"\"\n",
    "    sentences = seg_en.segment(text)\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 1]\n",
    "\n",
    "# Load LaBSE model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading LaBSE model on {device}...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\", device=device)\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06d1fb",
   "metadata": {},
   "source": [
    "## 3. Import Vecalign Functions\n",
    "\n",
    "Import the core functions from the vecalign repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4c6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1909,\n",
      "                 from /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages/numpy/_core/include/numpy/arrayobject.h:5,\n",
      "                 from /home/thienan/.pyxbld/temp.linux-x86_64-cpython-310/home/thienan/Documents/coding/zh-vn-mt/vecalign/dp_core.c:1138:\n",
      "/home/thienan/miniconda3/envs/mt/lib/python3.10/site-packages/numpy/_core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      "   17 | #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "      |  ^~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecalign functions imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import vecalign functions\n",
    "from dp_utils import make_alignment_types, vecalign, yield_overlaps\n",
    "\n",
    "print(\"Vecalign functions imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e2139",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions for Vecalign\n",
    "\n",
    "Vecalign requires:\n",
    "1. Overlap generation (concatenated sentence combinations)\n",
    "2. Embeddings for each overlap\n",
    "3. A mapping from sentences to their overlap embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c08c41e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_line(line):\n",
    "    \"\"\"Preprocess line same as vecalign does.\"\"\"\n",
    "    line = line.strip()\n",
    "    if len(line) == 0:\n",
    "        line = 'BLANK_LINE'\n",
    "    return line\n",
    "\n",
    "\n",
    "def layer(lines, num_overlaps, comb=' '):\n",
    "    \"\"\"\n",
    "    Make front-padded overlapping sentences (from vecalign).\n",
    "    \"\"\"\n",
    "    if num_overlaps < 1:\n",
    "        raise Exception('num_overlaps must be >= 1')\n",
    "    out = ['PAD', ] * min(num_overlaps - 1, len(lines))\n",
    "    for ii in range(len(lines) - num_overlaps + 1):\n",
    "        out.append(comb.join(lines[ii:ii + num_overlaps]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_doc_embedding_direct(sentences: List[str], num_overlaps: int = 4):\n",
    "    \"\"\"\n",
    "    Create document embedding matrix for vecalign directly.\n",
    "    Encodes all overlaps in a single batch for efficiency.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        num_overlaps: Maximum number of sentences to concatenate\n",
    "        \n",
    "    Returns:\n",
    "        vecs0: 3D numpy array (num_overlaps, len(sentences), embedding_dim)\n",
    "    \"\"\"\n",
    "    if not sentences:\n",
    "        return None\n",
    "    \n",
    "    # Preprocess sentences like vecalign does\n",
    "    lines = [preprocess_line(s) for s in sentences]\n",
    "    n_sents = len(lines)\n",
    "    \n",
    "    # Collect all overlaps we need to encode, with their positions\n",
    "    overlaps_to_encode = []\n",
    "    positions = []  # (overlap_idx, sent_idx)\n",
    "    \n",
    "    for ii, overlap in enumerate(range(1, num_overlaps + 1)):\n",
    "        layer_output = layer(lines, overlap)\n",
    "        for jj, out_line in enumerate(layer_output):\n",
    "            out_line = out_line[:10000]  # limit length\n",
    "            overlaps_to_encode.append(out_line)\n",
    "            positions.append((ii, jj))\n",
    "    \n",
    "    # Encode all overlaps in ONE batch call (much faster!)\n",
    "    if overlaps_to_encode:\n",
    "        all_embeddings = model.encode(\n",
    "            overlaps_to_encode, \n",
    "            convert_to_numpy=True, \n",
    "            show_progress_bar=False,\n",
    "            batch_size=64\n",
    "        )\n",
    "        # Ensure it's a numpy array with correct dtype\n",
    "        all_embeddings = np.asarray(all_embeddings, dtype=np.float32)\n",
    "        vecsize = all_embeddings.shape[1]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Create 3D array like vecalign expects\n",
    "    vecs0 = np.zeros((num_overlaps, n_sents, vecsize), dtype=np.float32)\n",
    "    \n",
    "    # Fill in the embeddings\n",
    "    for idx, (ii, jj) in enumerate(positions):\n",
    "        vecs0[ii, jj, :] = all_embeddings[idx]\n",
    "    \n",
    "    # Normalize vectors to unit length (required for vecalign cosine similarity)\n",
    "    for ii in range(num_overlaps):\n",
    "        for jj in range(n_sents):\n",
    "            norm = np.linalg.norm(vecs0[ii, jj, :])\n",
    "            if norm > 1e-8:\n",
    "                vecs0[ii, jj, :] = vecs0[ii, jj, :] / norm\n",
    "            else:\n",
    "                # Use small random vector if norm is too small\n",
    "                vecs0[ii, jj, :] = np.random.randn(vecsize).astype(np.float32)\n",
    "                vecs0[ii, jj, :] = vecs0[ii, jj, :] / np.linalg.norm(vecs0[ii, jj, :])\n",
    "    \n",
    "    return vecs0\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d72ee",
   "metadata": {},
   "source": [
    "## 5. Define Alignment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b073965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment function defined!\n"
     ]
    }
   ],
   "source": [
    "def align_zh_vi(zh_text: str, vi_text: str, \n",
    "                source: str = \"unknown\", \n",
    "                source_id: int = 0,\n",
    "                alignment_max_size: int = 4,\n",
    "                del_percentile_frac: float = 0.2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Aligns Chinese and Vietnamese text using Vecalign with LaBSE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zh_text : str\n",
    "        Chinese text to align\n",
    "    vi_text : str\n",
    "        Vietnamese text to align\n",
    "    source : str\n",
    "        Source identifier for the text pair\n",
    "    source_id : int\n",
    "        ID of the source document\n",
    "    alignment_max_size : int\n",
    "        Maximum sentences in one alignment (default: 4)\n",
    "    del_percentile_frac : float\n",
    "        Deletion penalty percentile (default: 0.2)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]: List of aligned sentence pairs with metadata\n",
    "    \"\"\"\n",
    "    from math import ceil\n",
    "    \n",
    "    # Split into sentences\n",
    "    zh_sents = split_chinese_sentences(zh_text)\n",
    "    vi_sents = split_vietnamese_sentences(vi_text)\n",
    "    \n",
    "    if not zh_sents or not vi_sents:\n",
    "        return []\n",
    "    \n",
    "    # Create document embeddings directly (single batch encode for each side)\n",
    "    vecs0 = make_doc_embedding_direct(zh_sents, alignment_max_size)\n",
    "    vecs1 = make_doc_embedding_direct(vi_sents, alignment_max_size)\n",
    "    \n",
    "    if vecs0 is None or vecs1 is None:\n",
    "        return []\n",
    "    \n",
    "    # Get alignment types\n",
    "    final_alignment_types = make_alignment_types(alignment_max_size)\n",
    "    \n",
    "    # Calculate search width\n",
    "    width_over2 = ceil(alignment_max_size / 2.0) + 5  # 5 is search_buffer_size\n",
    "    \n",
    "    # Run vecalign\n",
    "    stack = vecalign(\n",
    "        vecs0=vecs0,\n",
    "        vecs1=vecs1,\n",
    "        final_alignment_types=final_alignment_types,\n",
    "        del_percentile_frac=del_percentile_frac,\n",
    "        width_over2=width_over2,\n",
    "        max_size_full_dp=300,\n",
    "        costs_sample_size=20000,\n",
    "        num_samps_for_norm=100\n",
    "    )\n",
    "    \n",
    "    # Extract alignments\n",
    "    alignments = stack[0]['final_alignments']\n",
    "    scores = stack[0]['alignment_scores']\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    for (src_indices, tgt_indices), score in zip(alignments, scores):\n",
    "        if len(src_indices) > 0 and len(tgt_indices) > 0:\n",
    "            zh_aligned = ' '.join(zh_sents[i] for i in src_indices)\n",
    "            vi_aligned = ' '.join(vi_sents[i] for i in tgt_indices)\n",
    "            \n",
    "            results.append({\n",
    "                'zh': zh_aligned,\n",
    "                'vi': vi_aligned,\n",
    "                'source': str(source),\n",
    "                'source_id': int(source_id),\n",
    "                'align_type': f\"{len(src_indices)}-{len(tgt_indices)}\",\n",
    "                'score': float(score),\n",
    "                'src_indices': [int(i) for i in src_indices],\n",
    "                'tgt_indices': [int(i) for i in tgt_indices]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Alignment function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fa19a",
   "metadata": {},
   "source": [
    "## 6. Process Data from JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b30cffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path: str, output_json: str):\n",
    "    \"\"\"\n",
    "    Process a JSON file containing zh-vi text pairs and create aligned corpus.\n",
    "    Saves results incrementally to JSON after each successful alignment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_path : str\n",
    "        Path to input JSON file\n",
    "    output_json : str\n",
    "        Path to output JSON file (results saved incrementally)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]: All aligned pairs\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} documents\")\n",
    "    print(f\"Output: {output_json}\")\n",
    "    \n",
    "    all_pairs: List[Dict] = []\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Aligning documents\"):\n",
    "        if 'zh' not in item or 'vi' not in item:\n",
    "            continue\n",
    "        \n",
    "        zh_text = item['zh'].strip()\n",
    "        vi_text = item['vi'].strip()\n",
    "        \n",
    "        if not zh_text or not vi_text:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            aligned = align_zh_vi(\n",
    "                zh_text, vi_text,\n",
    "                source=item.get('source', 'unknown'),\n",
    "                source_id=item.get('id', 0),\n",
    "                alignment_max_size = 2\n",
    "            )\n",
    "            \n",
    "            if aligned:\n",
    "                all_pairs.extend(aligned)\n",
    "                \n",
    "                # Save incrementally after each successful alignment\n",
    "                with open(output_json, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(all_pairs, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {item.get('id', '?')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal aligned pairs: {len(all_pairs)}\")\n",
    "    print(f\"Saved to: {output_json}\")\n",
    "    \n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16331018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents\n",
      "Output: /home/thienan/Documents/coding/zh-vn-mt/corpus_vecalign.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents: 100%|██████████| 5/5 [01:09<00:00, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total aligned pairs: 172\n",
      "Saved to: /home/thienan/Documents/coding/zh-vn-mt/corpus_vecalign.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Change 'subsubset.json' to 'data.json' for full dataset\n",
    "\n",
    "all_pairs = process_data(\n",
    "    str(NOTEBOOK_DIR / 'subsubset.json'), \n",
    "    output_json=str(NOTEBOOK_DIR / 'corpus_vecalign_clone.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a700b7",
   "metadata": {},
   "source": [
    "## 7. Export to Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb50a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_corpus(all_pairs: List[Dict], output_prefix: str):\n",
    "    \"\"\"\n",
    "    Export aligned pairs to multiple formats: .zh, .vi, .tsv\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_pairs : List[Dict]\n",
    "        List of aligned sentence pairs\n",
    "    output_prefix : str\n",
    "        Prefix for output files\n",
    "    \"\"\"\n",
    "    # Save parallel text files\n",
    "    with open(f\"{output_prefix}.zh\", 'w', encoding='utf-8') as fz, \\\n",
    "         open(f\"{output_prefix}.vi\", 'w', encoding='utf-8') as fv:\n",
    "        for p in all_pairs:\n",
    "            fz.write(p['zh'] + '\\n')\n",
    "            fv.write(p['vi'] + '\\n')\n",
    "    \n",
    "    # Save TSV\n",
    "    with open(f\"{output_prefix}.tsv\", 'w', encoding='utf-8') as f:\n",
    "        f.write(\"zh\\tvi\\tscore\\talign_type\\tsource\\tsource_id\\n\")\n",
    "        for p in all_pairs:\n",
    "            zh_clean = p['zh'].replace('\\t', ' ').replace('\\n', ' ')\n",
    "            vi_clean = p['vi'].replace('\\t', ' ').replace('\\n', ' ')\n",
    "            f.write(f\"{zh_clean}\\t{vi_clean}\\t{p.get('score', 0):.4f}\\t{p['align_type']}\\t{p['source']}\\t{p['source_id']}\\n\")\n",
    "    \n",
    "    print(f\"Exported to: {output_prefix}.zh, {output_prefix}.vi, {output_prefix}.tsv\")\n",
    "\n",
    "# Export the results\n",
    "export_corpus(all_pairs, str(NOTEBOOK_DIR / 'corpus_vecalign'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32f83e",
   "metadata": {},
   "source": [
    "## 8. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print alignment statistics\n",
    "if all_pairs:\n",
    "    print(f\"Total aligned pairs: {len(all_pairs)}\")\n",
    "    \n",
    "    # Alignment type distribution\n",
    "    type_counts = {}\n",
    "    for p in all_pairs:\n",
    "        atype = p['align_type']\n",
    "        type_counts[atype] = type_counts.get(atype, 0) + 1\n",
    "    \n",
    "    print(\"\\nAlignment type distribution:\")\n",
    "    for atype, count in sorted(type_counts.items()):\n",
    "        pct = 100 * count / len(all_pairs)\n",
    "        print(f\"  {atype}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Score statistics\n",
    "    scores = [p.get('score', 0) for p in all_pairs]\n",
    "    print(f\"\\nScore statistics:\")\n",
    "    print(f\"  Min: {min(scores):.4f}\")\n",
    "    print(f\"  Max: {max(scores):.4f}\")\n",
    "    print(f\"  Mean: {np.mean(scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
