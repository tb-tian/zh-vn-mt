{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa97d2e",
   "metadata": {},
   "source": [
    "# Chinese-Vietnamese Sentence Alignment using Bertalign + SBERT\n",
    "\n",
    "## Features:\n",
    "- Uses LaBSE (Language-agnostic BERT Sentence Embedding) for multilingual sentence embeddings\n",
    "- Performs sentence segmentation for both Chinese and Vietnamese\n",
    "- Aligns sentences using Bertalign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98ba11",
   "metadata": {},
   "source": [
    "## 0. Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4931a5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.60.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu==1.7.2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu==1.7.2\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: numba==0.60.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.60.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu==1.7.2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu==1.7.2\u001b[0m\u001b[31m\n",
      "\u001b[0mObtaining file:///content/bertalign\n",
      "Obtaining file:///content/bertalign\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Installing collected packages: Bertalign\n",
      "  Attempting uninstall: Bertalign\n",
      "Installing collected packages: Bertalign\n",
      "  Attempting uninstall: Bertalign\n",
      "    Found existing installation: Bertalign 0.1.0\n",
      "    Uninstalling Bertalign-0.1.0:\n",
      "    Found existing installation: Bertalign 0.1.0\n",
      "    Uninstalling Bertalign-0.1.0:\n",
      "      Successfully uninstalled Bertalign-0.1.0\n",
      "  Running setup.py develop for Bertalign\n",
      "      Successfully uninstalled Bertalign-0.1.0\n",
      "  Running setup.py develop for Bertalign\n",
      "Successfully installed Bertalign-0.1.0\n",
      "Successfully installed Bertalign-0.1.0\n",
      "Installation complete. Please RESTART your runtime/kernel!\n",
      "Installation complete. Please RESTART your runtime/kernel!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('bertalign'):\n",
    "    !git clone https://github.com/bfsujason/bertalign.git\n",
    "\n",
    "os.chdir('bertalign')\n",
    "\n",
    "!pip install faiss-cpu\n",
    "\n",
    "if os.path.exists('requirements.txt'):\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "!pip install -e .\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "print(\"Installation complete. Please RESTART your runtime/kernel!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d47dc2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the notebook's directory and set it as working directory\n",
    "# This ensures relative paths work correctly regardless of where kernel started\n",
    "NOTEBOOK_DIR = Path(os.getcwd()).resolve()\n",
    "\n",
    "# If we're inside 'bertalign' folder from installation, go back up\n",
    "if NOTEBOOK_DIR.name == 'bertalign' and (NOTEBOOK_DIR.parent / 'bertalign_sbert_notebook.ipynb').exists():\n",
    "    NOTEBOOK_DIR = NOTEBOOK_DIR.parent\n",
    "    os.chdir(NOTEBOOK_DIR)\n",
    "\n",
    "print(f\"Working directory: {NOTEBOOK_DIR}\")\n",
    "\n",
    "# Add local bertalign package to path\n",
    "sys.path.insert(0, str(NOTEBOOK_DIR / 'bertalign'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0884ee",
   "metadata": {},
   "source": [
    "## 1. Create Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a25ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_json(input_json_path, start_id, end_id, rename_cn_to_zh=False, source_name=None):\n",
    "    \"\"\"\n",
    "    Loads and filters a JSON file by ID range.\n",
    "    \n",
    "    Args:\n",
    "        input_json_path (str): Path to the source JSON file.\n",
    "        start_id (int): The starting ID of the range (inclusive).\n",
    "        end_id (int): The ending ID of the range (inclusive).\n",
    "        rename_cn_to_zh (bool): If True, rename 'cn' key to 'zh'.\n",
    "        source_name (str): Name of the source to add to each item.\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered data items.\n",
    "    \"\"\"\n",
    "    print(f\"Loading '{input_json_path}' for IDs between {start_id} and {end_id}...\")\n",
    "    \n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Error: The root of the JSON file is not a list.\")\n",
    "        return []\n",
    "\n",
    "    # Filter the data based on the id range\n",
    "    filtered_data = [\n",
    "        item for item in data\n",
    "        if isinstance(item, dict) and 'id' in item and start_id <= item.get('id', -1) <= end_id\n",
    "    ]\n",
    "    \n",
    "    # Rename 'cn' to 'zh' if needed\n",
    "    if rename_cn_to_zh:\n",
    "        for item in filtered_data:\n",
    "            if 'cn' in item:\n",
    "                item['zh'] = item.pop('cn')\n",
    "    \n",
    "    # Add source name if provided\n",
    "    if source_name:\n",
    "        for item in filtered_data:\n",
    "            item['source'] = source_name\n",
    "    \n",
    "    print(f\"  -> Loaded {len(filtered_data)} items\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def combine_json_subsets(output_path, *sources):\n",
    "    \"\"\"\n",
    "    Combines multiple JSON sources into a single file.\n",
    "    \n",
    "    Args:\n",
    "        output_path (str): Path to save the combined JSON file.\n",
    "        *sources: Tuples of (input_path, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    for input_path, start_id, end_id, rename_cn_to_zh, source_name in sources:\n",
    "        items = load_and_filter_json(input_path, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "        combined_data.extend(items)\n",
    "    \n",
    "    # Write the combined data (keep original IDs)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(combined_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"\\nSuccessfully created '{output_path}' with {len(combined_data)} total items.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99daa4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data.json\"):\n",
    "    combine_json_subsets(\n",
    "        \"data.json\",\n",
    "        # (input_file, start_id, end_id, rename_cn_to_zh, source_name)\n",
    "        (\"json1.json\", 1, 212, True, \"json1\"),   # json1 uses 'cn' -> rename to 'zh'\n",
    "        (\"json2.json\", 1, 1163, False, \"json2\"), # json2 already uses 'zh'\n",
    "    )\n",
    "else:\n",
    "    print(\"data.json already exists, skipping creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e1b20",
   "metadata": {},
   "source": [
    "## 2. Patch Bertalign for Vietnamese Support\n",
    "\n",
    "The original Bertalign doesn't support Vietnamese. We need to patch:\n",
    "- Language detection (to identify Vietnamese text)\n",
    "- Sentence splitting (Vietnamese uses similar punctuation to English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50cb5f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches applied\n"
     ]
    }
   ],
   "source": [
    "# Import bertalign modules\n",
    "import sys\n",
    "sys.path.insert(0, 'bertalign')  # Add the repo root to path\n",
    "\n",
    "# Now import the submodules directly\n",
    "import bertalign.utils as bertalign_utils\n",
    "import bertalign.aligner as bertalign_aligner\n",
    "\n",
    "def patched_detect_lang(text):\n",
    "    \"\"\"\n",
    "    Simple language detection based on character ranges.\n",
    "    Returns 'zh' for Chinese, 'vi' for Vietnamese.\n",
    "    \"\"\"\n",
    "    # Count Chinese characters\n",
    "    chinese_chars = sum(1 for c in text if '\\u4e00' <= c <= '\\u9fff')\n",
    "    # Count Vietnamese diacritics\n",
    "    vietnamese_chars = sum(1 for c in text if c in 'àáạảãăắằặẳẵâấầậẩẫèéẹẻẽêếềệểễìíịỉĩòóọỏõôốồộổỗơớờợởỡùúụủũưứừựửữỳýỵỷỹđÀÁẠẢÃĂẮẰẶẲẴÂẤẦẬẨẪÈÉẸẺẼÊẾỀỆỂỄÌÍỊỈĨÒÓỌỎÕÔỐỒỘỔỖƠỚỜỢỞỠÙÚỤỦŨƯỨỪỰỬỮỲÝỴỶỸĐ')\n",
    "    \n",
    "    if chinese_chars > len(text) * 0.1:\n",
    "        return 'zh'\n",
    "    elif vietnamese_chars > 0:\n",
    "        return 'vi'\n",
    "    else:\n",
    "        return 'zh' if chinese_chars > vietnamese_chars else 'vi'\n",
    "\n",
    "def patched_split_sents(text, lang):\n",
    "    \"\"\"\n",
    "    Split text into sentences. Adds Vietnamese support.\n",
    "    \"\"\"\n",
    "    if lang == 'zh':\n",
    "        # Use the original Chinese splitter\n",
    "        return bertalign_utils._split_zh(text)\n",
    "    elif lang == 'vi':\n",
    "        # Vietnamese sentence splitting using punctuation\n",
    "        text = re.sub(r'([.?!])\\s+', r'\\1\\n', text)\n",
    "        sents = [s.strip() for s in text.split('\\n') if s.strip()]\n",
    "        return sents\n",
    "    elif lang in bertalign_utils.LANG.SPLITTER:\n",
    "        from sentence_splitter import SentenceSplitter\n",
    "        splitter = SentenceSplitter(language=lang)\n",
    "        sents = splitter.split(text=text)\n",
    "        sents = [sent.strip() for sent in sents]\n",
    "        return sents\n",
    "    else:\n",
    "        raise Exception(f'The language {lang} is not supported yet.')\n",
    "\n",
    "# Add Vietnamese to LANG.SPLITTER and LANG.ISO\n",
    "bertalign_utils.LANG.SPLITTER['vi'] = 'Vietnamese'\n",
    "bertalign_utils.LANG.ISO['vi'] = 'Vietnamese'\n",
    "\n",
    "# Apply the patches\n",
    "bertalign_utils.detect_lang = patched_detect_lang\n",
    "bertalign_utils.split_sents = patched_split_sents\n",
    "bertalign_aligner.detect_lang = patched_detect_lang\n",
    "bertalign_aligner.split_sents = patched_split_sents\n",
    "\n",
    "print(\"Patches applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608aa27d",
   "metadata": {},
   "source": [
    "## 3. Import Bertalign\n",
    "\n",
    "Now we can import the patched Bertalign. It uses LaBSE for sentence embeddings by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e524172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertalign imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from bertalign import Bertalign\n",
    "\n",
    "print(\"Bertalign imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d24c77d",
   "metadata": {},
   "source": [
    "## 4. Define Alignment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d43d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_zh_vi(zh_text: str, vi_text: str, \n",
    "                source: str = \"unknown\", \n",
    "                source_id: int = 0,\n",
    "                max_align: int = 5,\n",
    "                top_k: int = 3,\n",
    "                win: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Aligns Chinese and Vietnamese text using Bertalign with LaBSE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zh_text : str\n",
    "        Chinese text to align\n",
    "    vi_text : str\n",
    "        Vietnamese text to align\n",
    "    source : str\n",
    "        Source identifier for the text pair\n",
    "    source_id : int\n",
    "        ID of the source document\n",
    "    max_align : int\n",
    "        Maximum number of sentences to merge in alignment (default: 5)\n",
    "    top_k : int\n",
    "        Number of top candidates to consider (default: 3)\n",
    "    win : int\n",
    "        Window size for alignment search (default: 5)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]: List of aligned sentence pairs with metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Bertalign\n",
    "    aligner = Bertalign(\n",
    "        src=zh_text,\n",
    "        tgt=vi_text,\n",
    "        max_align=max_align,\n",
    "        top_k=top_k,\n",
    "        win=win,\n",
    "        skip=-0.1,\n",
    "        margin=True,\n",
    "        len_penalty=True,\n",
    "        is_split=False,\n",
    "    )\n",
    "    \n",
    "    # Perform alignment\n",
    "    aligner.align_sents()\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    for src_indices, tgt_indices in aligner.result:\n",
    "        if len(src_indices) > 0 and len(tgt_indices) > 0:\n",
    "            zh_aligned = ' '.join(aligner.src_sents[src_indices[0]:src_indices[-1]+1])\n",
    "            vi_aligned = ' '.join(aligner.tgt_sents[tgt_indices[0]:tgt_indices[-1]+1])\n",
    "            \n",
    "            results.append({\n",
    "                'zh': zh_aligned,\n",
    "                'vi': vi_aligned,\n",
    "                'source': str(source),\n",
    "                'source_id': int(source_id),  # Convert numpy int64 to Python int\n",
    "                'align_type': f\"{len(src_indices)}-{len(tgt_indices)}\",\n",
    "                'src_indices': [int(i) for i in src_indices],  # Convert numpy ints\n",
    "                'tgt_indices': [int(i) for i in tgt_indices]   # Convert numpy ints\n",
    "            })\n",
    "    \n",
    "    return results, aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740dd6b",
   "metadata": {},
   "source": [
    "## 6. Process Data from JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87eb9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path: str, output_json: str):\n",
    "    \"\"\"\n",
    "    Process a JSON file containing zh-vi text pairs and create aligned corpus.\n",
    "    Saves results incrementally to JSON after each successful alignment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_path : str\n",
    "        Path to input JSON file\n",
    "    output_json : str\n",
    "        Path to output JSON file (results saved incrementally)\n",
    "    limit : int\n",
    "        Maximum number of documents to process (optional)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Dict]: All aligned pairs\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    \n",
    "    print(f\"Loaded {len(data)} documents\")\n",
    "    print(f\"Output: {output_json}\")\n",
    "    \n",
    "    all_pairs: List[Dict] = []\n",
    "    \n",
    "    for item in tqdm(data, desc=\"Aligning documents\"):\n",
    "        if 'zh' not in item or 'vi' not in item:\n",
    "            continue\n",
    "        \n",
    "        zh_text = item['zh'].strip()\n",
    "        vi_text = item['vi'].strip()\n",
    "        \n",
    "        if not zh_text or not vi_text:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            aligned, _ = align_zh_vi(\n",
    "                zh_text, vi_text,\n",
    "                source=item.get('source', 'unknown'),\n",
    "                source_id=item.get('id', 0)\n",
    "            )\n",
    "            \n",
    "            if aligned:\n",
    "                all_pairs.extend(aligned)\n",
    "                \n",
    "                # Save incrementally after each successful alignment\n",
    "                with open(output_json, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(all_pairs, f, ensure_ascii=False, indent=4)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {item.get('id', '?')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal aligned pairs: {len(all_pairs)}\")\n",
    "    print(f\"Saved to: {output_json}\")\n",
    "    \n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f6cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Bertalign Sentence Alignment\n",
      "============================================================\n",
      "Loaded 5 documents\n",
      "Output: /home/thienan/Documents/coding/zh-vn-mt/corpus_bertalign.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source language: Chinese, Number of sentences: 43\n",
      "Target language: Vietnamese, Number of sentences: 54\n",
      "Embedding source and target text using LaBSE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents:  20%|██        | 1/5 [00:27<01:49, 27.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing first-step alignment ...\n",
      "Performing second-step alignment ...\n",
      "Finished! Successfully aligning 43 Chinese sentences to 54 Vietnamese sentences\n",
      "\n",
      "Source language: Chinese, Number of sentences: 44\n",
      "Target language: Vietnamese, Number of sentences: 50\n",
      "Embedding source and target text using LaBSE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents:  40%|████      | 2/5 [00:55<01:24, 28.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing first-step alignment ...\n",
      "Performing second-step alignment ...\n",
      "Finished! Successfully aligning 44 Chinese sentences to 50 Vietnamese sentences\n",
      "\n",
      "Source language: Chinese, Number of sentences: 42\n",
      "Target language: Vietnamese, Number of sentences: 46\n",
      "Embedding source and target text using LaBSE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents:  60%|██████    | 3/5 [01:21<00:53, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing first-step alignment ...\n",
      "Performing second-step alignment ...\n",
      "Finished! Successfully aligning 42 Chinese sentences to 46 Vietnamese sentences\n",
      "\n",
      "Source language: Chinese, Number of sentences: 36\n",
      "Target language: Vietnamese, Number of sentences: 47\n",
      "Embedding source and target text using LaBSE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents:  80%|████████  | 4/5 [01:45<00:25, 25.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing first-step alignment ...\n",
      "Performing second-step alignment ...\n",
      "Finished! Successfully aligning 36 Chinese sentences to 47 Vietnamese sentences\n",
      "\n",
      "Source language: Chinese, Number of sentences: 61\n",
      "Target language: Vietnamese, Number of sentences: 68\n",
      "Embedding source and target text using LaBSE ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning documents: 100%|██████████| 5/5 [02:17<00:00, 27.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing first-step alignment ...\n",
      "Performing second-step alignment ...\n",
      "Finished! Successfully aligning 61 Chinese sentences to 68 Vietnamese sentences\n",
      "\n",
      "\n",
      "Total aligned pairs: 207\n",
      "Saved to: /home/thienan/Documents/coding/zh-vn-mt/corpus_bertalign.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Change 'subsubset.json' to 'data.json' for full dataset\n",
    "\n",
    "all_pairs = process_data(\n",
    "    str(NOTEBOOK_DIR / 'subsubset.json'), \n",
    "    output_json=str(NOTEBOOK_DIR / 'corpus_bertalign.json')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
